import { DataSource } from 'typeorm';
import { Model, ModelVariant } from '../database/entities';

async function cleanupDuplicateModels() {
  const dataSource = new DataSource({
    type: 'postgres', // Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð½Ð° Ð²Ð°Ñˆ Ñ‚Ð¸Ð¿ Ð‘Ð”
    host: process.env.DB_HOST || 'localhost',
    port: parseInt(process.env.DB_PORT || '5432'),
    username: process.env.DB_USERNAME || 'postgres',
    password: process.env.DB_PASSWORD || 'password',
    database: process.env.DB_DATABASE || 'parser_db',
    entities: [Model, ModelVariant],
    synchronize: false,
  });

  await dataSource.initialize();
  
  try {
    const modelRepository = dataSource.getRepository(Model);
    const modelVariantRepository = dataSource.getRepository(ModelVariant);

    console.log('ðŸ” Searching for duplicate models with same normalizedName...');

    // ÐÐ°Ð¹Ñ‚Ð¸ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð¿Ð¾ normalizedName
    const duplicatesQuery = `
      SELECT normalized_name, array_agg(id) as model_ids, count(*) as count
      FROM models 
      WHERE normalized_name IS NOT NULL
      GROUP BY normalized_name 
      HAVING count(*) > 1
    `;

    const duplicates = await dataSource.query(duplicatesQuery);
    
    console.log(`ðŸ“Š Found ${duplicates.length} normalized names with duplicates`);

    for (const duplicate of duplicates) {
      const { normalized_name, model_ids, count } = duplicate;
      console.log(`\nðŸ”§ Processing "${normalized_name}" with ${count} duplicates`);

      // Ð’Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ "Ð¼Ð°ÑÑ‚ÐµÑ€" Ð¼Ð¾Ð´ÐµÐ»ÑŒ (Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¿Ð¾ ID)
      const [masterId, ...duplicateIds] = model_ids;
      console.log(`   Master model ID: ${masterId}`);
      console.log(`   Duplicate IDs: ${duplicateIds.join(', ')}`);

      // ÐŸÐµÑ€ÐµÐ½ÐµÑÑ‚Ð¸ Ð²ÑÐµ ModelVariants Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð½Ð° Ð¼Ð°ÑÑ‚ÐµÑ€ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
      for (const duplicateId of duplicateIds) {
        const variantsCount = await modelVariantRepository.update(
          { modelId: duplicateId },
          { modelId: masterId }
        );
        console.log(`   Moved ${variantsCount.affected} variants from ${duplicateId} to ${masterId}`);
      }

      // Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
      await modelRepository.delete(duplicateIds);
      console.log(`   âœ… Deleted ${duplicateIds.length} duplicate models`);
    }

    console.log('\nðŸŽ‰ Cleanup completed successfully!');
  } catch (error) {
    console.error('âŒ Error during cleanup:', error);
    throw error;
  } finally {
    await dataSource.destroy();
  }
}

// Ð—Ð°Ð¿ÑƒÑÐº ÐµÑÐ»Ð¸ Ñ„Ð°Ð¹Ð» Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ
if (require.main === module) {
  cleanupDuplicateModels()
    .then(() => {
      console.log('âœ¨ Script finished');
      process.exit(0);
    })
    .catch((error) => {
      console.error('ðŸ’¥ Script failed:', error);
      process.exit(1);
    });
}

export { cleanupDuplicateModels };